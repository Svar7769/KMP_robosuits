# Robotic Task Learning using Kernelized Movement Primitives (KMP)

This project implements the Kernelized Movement Primitives (KMP) algorithm for a robotic pick-and-place task within the Robosuite simulation environment. The system learns three fundamental skills—reaching, transporting, and retreating—from human demonstrations and then executes them to manipulate objects in the simulation.

## Overview

The project is structured into two main phases:
1.  **Demonstration and Learning:** Trajectory demonstrations for the `reach`, `transport`, and `retreat` skills are collected using a joystick. These trajectories are then processed (smoothed and normalized) and used to train a KMP model.
2.  **Execution and Simulation:** The trained KMP models are combined with a symbolic planner (using PDDL) to execute a complete pick-and-place task, such as moving four cubes into four designated bins.

This implementation leverages the `robosuite` simulation framework, the `kmp` library for learning, and `pyperplan` for symbolic planning.

## Features

-   **Demonstration Recording:** An interactive script to record robot end-effector trajectories using a joystick in the `robosuite` environment.
-   **Data Processing:** Scripts to normalize trajectory lengths and apply smoothing filters (moving average) to the recorded data.
-   **KMP Training:** Implementation of KMP training using a Gaussian Mixture Model (GMM) to create a probabilistic representation of the demonstrated skills.
-   **Task Execution:** A simulation script that loads the trained KMP models and executes a sequence of skills to perform a pick-and-place task.
-   **Visualization:** Includes scripts to visualize the recorded and learned trajectories in both 2D and 3D space, which helps in debugging and analyzing the learned motions.

## Methodology

The workflow of this project is as follows:

1.  **Data Collection:** Human demonstrations for each of the three skills (reach, transport, retreat) are recorded. The end-effector's 3D position is saved for each demonstration.
2.  **Data Preprocessing:**
    * **Normalization:** All recorded trajectories are normalized to a fixed length (e.g., 200 time steps) using linear interpolation.
    * **Smoothing:** A moving average filter is applied to the normalized trajectories to reduce noise and jitter from the joystick input.
3.  **KMP Model Training:**
    * A Gaussian Mixture Model (GMM) is trained on the processed trajectories to create a probabilistic model of the demonstrated skills.
    * This GMM is then used to initialize a Kernelized Movement Primitive (KMP) model, which learns a flexible and generalizable representation of the motion.
4.  **Simulation and Execution:**
    * The trained KMP model is loaded into the `robosuite` simulation environment.
    * A high-level task planner (not fully implemented in the provided notebook but described in the report) would generate a sequence of skills.
    * A simple controller then guides the robot's end-effector along the trajectory generated by the KMP model to execute the task.

## Getting Started

### Prerequisites

-   Python 3.8+
-   MuJoCo 2.1.0 or higher
-   An Xbox controller or a similar joystick recognized by `pygame`.
-   The following Python libraries:
    -   `robosuite`
    -   `numpy`
    -   `h5py`
    -   `matplotlib`
    -   `pygame`
    -   `kmp` (from the `KMP_demos` repository or a similar implementation)

### Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/Svar7769/KMP-robosuite.git](https://github.com/Svar7769/KMP-robosuite.git)
    cd KMP-robosuite
    ```

2.  **Set up a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
    ```

3.  **Install the required packages:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Install MuJoCo:** Follow the official installation instructions for your operating system from the [MuJoCo website](https://mujoco.org/).

## Usage

The project is structured as a Jupyter Notebook (`KMP_Spot.ipynb`) which is divided into several cells. Run the cells in order to perform the full workflow.

1.  **Record Demonstrations:**
    -   Run the first cell in the notebook.
    -   Use the joystick to perform and record four demonstrations of a reaching task.

2.  **Process Data:**
    -   Run the second cell to normalize and smooth the recorded demonstrations. This will generate the `normalized_kmp_dataset.hdf5` file.

3.  **Visualize Data (Optional):**
    -   Run the third cell to see a 3D plot of the processed trajectories to ensure they are consistent.

4.  **Train the KMP Model:**
    -   Run the fourth cell to train the KMP model on the processed data. This will save the trained model to `trained_kmp_model.h5` and display a plot comparing the learned trajectory to the original demonstrations.

5.  **Run in Simulation:**
    -   Run the final cell to execute the learned trajectory in the `robosuite` simulation.

## References

-   Huang, Y., Rozo, L., Silvério, J., & Caldwell, D. G. (2019). Kernelized movement primitives. *The International Journal of Robotics Research*, 38(7), 833-852.
-   Zhu, Y., Gupta, A., Abbeel, P., Fei-Fei, L., & Savarese, S. (2020). robosuite: A modular simulation framework and benchmark for robot learning.
-   Busellato, L. (2022). KMP Demos: Kernelized Movement Primitives. GitHub repository.


